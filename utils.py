# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

import random, torch, os, json
import numpy as np

from rouge_score import rouge_scorer
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')


class Config:
    # to access a dict with object.key
    def __init__(self, dictionary):
        self.__dict__ = dictionary


def set_seed(seed_value):
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    os.environ["PYTHONHASHSEED"] = str(seed_value)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def evaluate_nlg(predictions, references):
    # Initialize metrics
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    smooth = SmoothingFunction().method1
    
    # Compute scores
    rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}
    bleu_scores = 0
    
    for pred, ref in zip(predictions, references):
        # Compute ROUGE
        rouge_result = scorer.score(ref, pred)
        for key in rouge_scores:
            rouge_scores[key] += rouge_result[key].fmeasure
        
        # Compute BLEU
        ref_tokens = nltk.word_tokenize(ref.lower())
        pred_tokens = nltk.word_tokenize(pred.lower())
        bleu_scores += sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smooth)
    
    # Average scores
    total = len(predictions)
    for key in rouge_scores:
        rouge_scores[key] /= total
    bleu_scores /= total
    
    return {'rouge': rouge_scores, 'bleu': bleu_scores}

def call_llm_api(prompt):
    from openai import AzureOpenAI
    from keyconfig import AZURE_API_KEY, AZURE_ENDPOINT
    client = AzureOpenAI(api_key=AZURE_API_KEY, api_version="2024-08-01-preview", azure_endpoint=AZURE_ENDPOINT)
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are an expert evaluator of explanations."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.1,  # Low temperature for more consistent evaluations
        max_tokens=1000
    )

    return response.choices[0].message.content


def evaluate_with_llm(question, explanation):
    evaluation_prompt = f"""You are an expert evaluator of explanations meant for beginners. 
    You will be given a question and an explanation generated by an AI. 
    Evaluate the explanation based on the following criteria:
    
    QUESTION: {question}
    EXPLANATION: {explanation}
    
    Please evaluate on a scale of 1-5 (where 5 is best) for each of these criteria:
    
    1. CLARITY & SIMPLICITY: Is the explanation easy to understand for someone without technical knowledge?
    2. ACCURACY: Is the information provided factually correct?
    3. COMPLETENESS: Does the explanation cover the main aspects needed to answer the question?
    4. ENGAGEMENT: Is the explanation presented in an engaging way appropriate for a beginner?
    5. CONCISENESS: Is the explanation appropriately concise while being thorough?
    
    For each criterion, provide:
    - Score (1-5)
    - 1-2 sentence justification for your score
    
    Then provide a TOTAL SCORE (average of all scores) and a brief OVERALL ASSESSMENT of the explanation's quality.
    
    Your output must be a valid JSON with the following structure:
    {{
      "clarity_score": int,
      "clarity_justification": "string",
      "accuracy_score": int,
      "accuracy_justification": "string",
      "completeness_score": int,
      "completeness_justification": "string",
      "engagement_score": int,
      "engagement_justification": "string",
      "conciseness_score": int,
      "conciseness_justification": "string",
      "total_score": float,
      "overall_assessment": "string"
    }}
    """
    try:
        response = call_llm_api(evaluation_prompt)
    except:
        # OpenAI request block, or out of credits, ig
        return None
    try:
        output = json.loads(response)
    except json.decoder.JSONDecodeError:
        cleaned_string = response.strip("```json").strip("```")
        output = json.loads(cleaned_string)
    except Exception as e:
        return None

    return output